ampere_fp16_s16816gemm_fp16_128x128_ldg8_f2f_stages_32x5_nn
ampere_fp16_s16816gemm_fp16_128x128_ldg8_f2f_stages_32x5_nt
ampere_fp16_s16816gemm_fp16_128x128_ldg8_f2f_stages_32x5_tn
ampere_fp16_s16816gemm_fp16_128x128_ldg8_f2f_stages_64x3_nt
ampere_fp16_s16816gemm_fp16_128x128_ldg8_relu_f2f_stages_32x5_tn
ampere_fp16_s16816gemm_fp16_128x256_ldg8_f2f_stages_64x3_nn
ampere_fp16_s16816gemm_fp16_128x256_ldg8_f2f_stages_64x3_nt
ampere_fp16_s16816gemm_fp16_128x256_ldg8_f2f_stages_64x3_tn
ampere_fp16_s16816gemm_fp16_128x256_ldg8_relu_f2f_stages_64x3_tn
ampere_fp16_s16816gemm_fp16_128x64_ldg8_f2f_nn
ampere_fp16_s16816gemm_fp16_128x64_ldg8_f2f_stages_32x6_nn
ampere_fp16_s16816gemm_fp16_128x64_ldg8_f2f_stages_32x6_nt
ampere_fp16_s16816gemm_fp16_128x64_ldg8_f2f_stages_32x6_tn
ampere_fp16_s16816gemm_fp16_128x64_ldg8_f2f_stages_64x3_nn
ampere_fp16_s16816gemm_fp16_128x64_ldg8_f2f_stages_64x3_nt
ampere_fp16_s16816gemm_fp16_128x64_ldg8_f2f_stages_64x4_nt
ampere_fp16_s16816gemm_fp16_128x64_ldg8_relu_f2f_stages_32x6_tn
ampere_fp16_s16816gemm_fp16_128x64_ldg8_relu_f2f_stages_64x4_tn
ampere_fp16_s16816gemm_fp16_256x128_ldg8_f2f_stages_32x3_nn
ampere_fp16_s16816gemm_fp16_256x128_ldg8_f2f_stages_32x3_nt
ampere_fp16_s16816gemm_fp16_256x128_ldg8_f2f_stages_32x3_tn
ampere_fp16_s16816gemm_fp16_256x128_ldg8_f2f_stages_64x3_nn
ampere_fp16_s16816gemm_fp16_256x128_ldg8_f2f_stages_64x3_nt
ampere_fp16_s16816gemm_fp16_256x128_ldg8_relu_f2f_stages_32x3_tn
ampere_fp16_s16816gemm_fp16_256x128_ldg8_relu_f2f_stages_64x3_tn
ampere_fp16_s16816gemm_fp16_256x64_ldg8_f2f_stages_64x3_nt
ampere_fp16_s16816gemm_fp16_256x64_ldg8_relu_f2f_stages_64x3_tn
ampere_fp16_s16816gemm_fp16_64x128_ldg8_f2f_stages_32x6_nn
ampere_fp16_s16816gemm_fp16_64x128_ldg8_f2f_stages_32x6_tn
ampere_fp16_s16816gemm_fp16_64x128_ldg8_f2f_stages_64x4_nt
ampere_fp16_s16816gemm_fp16_64x128_sliced1x2_ldg8_f2f_stages_64x3_nn
ampere_fp16_s16816gemm_fp16_64x128_sliced1x2_ldg8_f2f_stages_64x3_tn
ampere_fp16_s16816gemm_fp16_64x64_ldg8_f2f_stages_64x5_tn
ampere_fp16_s16816gemm_fp16_64x64_ldg8_f2f_stages_64x6_tn
ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f_stages_64x5_nn
ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f_stages_64x5_nt
ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f_stages_64x5_tn
ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f_stages_64x6_nn
ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_f2f_stages_64x6_tn
ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_relu_f2f_stages_64x5_tn
ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_nn
ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_32x1_nt
ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_tn
ampere_fp16_s1688gemm_fp16_128x256_ldg8_f2f_stages_32x1_nt
ampere_fp16_s1688gemm_fp16_128x256_ldg8_f2f_stages_32x1_tn
ampere_fp16_s1688gemm_fp16_256x64_ldg8_f2f_nn
ampere_fp16_s1688gemm_fp16_256x64_ldg8_f2f_nt
ampere_fp16_s1688gemm_fp16_256x64_ldg8_f2f_stages_32x1_nn
ampere_fp16_s1688gemm_fp16_256x64_ldg8_f2f_tn
ampere_fp16_s1688gemm_fp16_64x128_sliced1x2_ldg8_f2f_nt
ampere_fp16_sgemm_fp16_128x32_nn
ampere_fp16_sgemm_fp16_128x64_tn
ampere_fp16_sgemm_fp16_32x128_nn
ampere_fp16_sgemm_fp16_32x128_tn
ampere_fp16_sgemm_fp16_64x64_nt
ampere_fp16_sgemm_fp16_64x64_tn
ampere_s16816gemm_fp16_128x128_ldg8_stages_32x5_tn
ampere_s16816gemm_fp16_128x64_ldg8_stages_64x4_nt
ampere_s16816gemm_fp16_64x64_ldg8_stages_64x5_nn
ampere_s16816gemm_fp16_64x64_ldg8_stages_64x5_nt
ampere_s16816gemm_fp16_64x64_ldg8_stages_64x6_tn
ampere_s16816gemm_fp16_64x64_sliced1x2_ldg8_stages_64x5_tn
ampere_s16816gemm_fp16_64x64_sliced1x2_ldg8_stages_64x6_nn
ampere_s16816gemm_fp16_64x64_sliced1x2_ldg8_stages_64x6_nt
cudnn_maxwell_fp16_scudnn_fp16_128x128_3dconv_fprop_medium_nn_v0
cudnn_maxwell_fp16_scudnn_fp16_128x128_3dconv_fprop_small_nn_v0
cudnn_maxwell_fp16_scudnn_fp16_128x128_stridedB_small_nn_v0
cudnn_maxwell_fp16_scudnn_fp16_128x128_stridedB_splitK_interior_nn_v0
cudnn_maxwell_fp16_scudnn_fp16_128x32_3dconv_fprop_medium_nn_v0
cudnn_maxwell_fp16_scudnn_fp16_128x32_3dconv_fprop_small_nn_v0
cudnn_maxwell_fp16_scudnn_fp16_128x32_stridedB_small_nn_v0
cudnn_maxwell_fp16_scudnn_fp16_128x32_stridedB_splitK_interior_nn_v0
cudnn_maxwell_fp16_scudnn_fp16_128x64_3dconv_fprop_medium_nn_v0
cudnn_maxwell_fp16_scudnn_fp16_128x64_stridedB_small_nn_v0
cudnn_maxwell_fp16_scudnn_fp16_128x64_stridedB_splitK_interior_nn_v0
cudnn_maxwell_gcgemm_64x64_nt_batched
cudnn_volta_fp16_s884cudnn_fp16_128x128_ldg8_dgrad_f2f_exp_medium_nhwc2nchw_tt_v1
cudnn_volta_fp16_s884cudnn_fp16_128x128_ldg8_dgrad_f2f_exp_small_nhwc_tt_v1
cudnn_volta_fp16_s884cudnn_fp16_128x128_ldg8_splitK_dgrad_f2f_exp_small_nhwc_tt_v1
cudnn_volta_fp16_s884cudnn_fp16_256x128_ldg8_dgrad_f2f_exp_medium_nhwc_tt_v1
cudnn_volta_fp16_s884cudnn_fp16_256x128_ldg8_dgrad_f2f_exp_medium_nhwc2nchw_tt_v1
cudnn_volta_fp16_s884cudnn_fp16_256x128_ldg8_dgrad_f2f_exp_small_nhwc_tt_v1
cudnn_volta_fp16_s884cudnn_fp16_256x64_ldg8_dgrad_f2f_exp_small_nhwc2nchw_tt_v1
cudnn_volta_fp16_s884cudnn_fp16_256x64_ldg8_splitK_relu_f2f_exp_interior_nhwc_tn_v1
cudnn_volta_fp16_scudnn_fp16_128x128_3dconv_fprop_medium_nn_v1
cudnn_volta_s884cudnn_fp16_128x128_ldg8_wgrad_idx_exp_interior_nhwc_nt_v1
cudnn_volta_s884cudnn_fp16_256x128_ldg8_wgrad_idx_exp_interior_nhwc_nt_v1
cudnn::cnn::dgrad_1x1_stride_2x2(cudnn::cnn::Dgrad2d_First_Layer_884_NHWC_Engine_Impl::Dgrad1x1Stride2x2Params)
hgemm_128x128x8_NN
hgemm_128x128x8_NN_vec
hgemm_128x128x8_NT_vec
hgemm_128x128x8_TN_vec
hgemm_32x32x32_NN
hgemm_32x32x32_NN_vec
hgemm_32x32x32_NT_vec
hgemm_32x32x32_TN
hgemm_32x32x32_TN_vec
hgemm_32x64x32_NN_vec
maxwell_fp16_scudnn_fp16_128x128_3dconv_fprop_small_nn_v0
maxwell_fp16_scudnn_fp16_128x128_stridedB_small_nn
maxwell_fp16_scudnn_fp16_128x128_stridedB_small_nn_v0
maxwell_fp16_scudnn_fp16_128x128_stridedB_splitK_interior_nn_v0
maxwell_fp16_scudnn_fp16_128x128_stridedB_splitK_large_nn
maxwell_fp16_scudnn_fp16_128x32_stridedB_small_nn_v0
maxwell_fp16_scudnn_fp16_128x32_stridedB_splitK_interior_nn_v0
maxwell_fp16_scudnn_fp16_128x32_stridedB_splitK_large_nn
maxwell_fp16_scudnn_fp16_128x32_stridedB_splitK_large_nn_v0
maxwell_fp16_scudnn_fp16_128x64_stridedB_small_nn_v0
maxwell_fp16_scudnn_fp16_128x64_stridedB_splitK_interior_nn_v0
maxwell_fp16_scudnn_fp16_128x64_stridedB_splitK_large_nn_v0
maxwell_fp16_scudnn_fp16_128x64_stridedB_splitK_medium_nn_v0
maxwell_fp16_sgemm_fp16_128x128_nn
maxwell_fp16_sgemm_fp16_128x128_tn
maxwell_fp16_sgemm_fp16_128x32_nn
maxwell_fp16_sgemm_fp16_128x32_nt
maxwell_fp16_sgemm_fp16_128x32_tn
maxwell_fp16_sgemm_fp16_128x64_nn
maxwell_fp16_sgemm_fp16_128x64_nt
maxwell_fp16_sgemm_fp16_128x64_tn
maxwell_fp16_sgemm_fp16_128x64_tt
maxwell_fp16_sgemm_fp16_32x128_nn
maxwell_fp16_sgemm_fp16_32x128_nt
maxwell_fp16_sgemm_fp16_32x128_tn
maxwell_fp16_sgemm_fp16_64x64_nn
maxwell_fp16_sgemm_fp16_64x64_nt
maxwell_fp16_sgemm_fp16_64x64_tn
maxwell_gcgemm_32x32_nt
maxwell_gcgemm_64x64_nt
maxwell_sgemm_32x128_nn
maxwell_sgemm_32x128_nt
maxwell_sgemm_64x64_nn
maxwell_sgemm_64x64_nt
maxwell_sgemm_fp16_128x32_nn
maxwell_sgemm_fp16_128x32_nt
maxwell_sgemm_fp16_128x32_tn
maxwell_sgemm_fp16_64x64_nn
maxwell_sgemm_fp16_64x64_tn
sm70_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r3s3_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x4_aligna2_alignc2_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x4_aligna8_alignc2_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x4_alignc2_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x4_alignc2_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x4_alignc2_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor8x8x4_aligna2_alignc8_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor8x8x4_aligna4_alignc2_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor8x8x4_aligna4_alignc8_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor8x8x4_aligna8_alignc8_execute_kernel_cudnn
sm70_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x4_alignc2_execute_kernel_cudnn
sm70_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x2_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x2_g1_tensor8x8x4_execute_split_k_kernel_cudnn
sm70_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize32x256x32_stage1_warpsize1x4x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x2x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x2x1_g1_tensor8x8x4_execute_split_k_kernel_cudnn
sm70_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_execute_kernel_cudnn
sm70_xmma_wgrad_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize16x64x64_stage1_warpsize1x4x1_g1_tensor8x8x4_aligna2_alignc2_execute_kernel_cudnn
sm70_xmma_wgrad_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize16x64x64_stage1_warpsize1x4x1_g1_tensor8x8x4_aligna4_alignc4_execute_kernel_cudnn
sm70_xmma_wgrad_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize32x16x64_stage1_warpsize2x1x1_g1_tensor8x8x4_aligna2_alignc2_execute_kernel_cudnn
sm70_xmma_wgrad_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize32x16x64_stage1_warpsize2x1x1_g1_tensor8x8x4_aligna4_alignc4_execute_kernel_cudnn
sm70_xmma_wgrad_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize32x32x64_stage1_warpsize2x2x1_g1_tensor8x8x4_aligna2_alignc2_execute_kernel_cudnn
sm70_xmma_wgrad_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize32x32x64_stage1_warpsize2x2x1_g1_tensor8x8x4_aligna2_alignc2_execute_split_k_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x256x64_stage3_warpsize1x4x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel_cudnn
sm80_xmma_dgrad_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_execute_kernel_cudnn
sm80_xmma_fprop_image_first_layer_f16f16_f32_f16_nhwckrsc_nhwc_hmma_k64c4r7s7_stride2x2_tile16x64x64_tensor1688_execute_kernel_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nchw_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x16_aligna2_alignc2_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x16_aligna4_alignc2_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x16_aligna8_alignc2_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x16_alignc2_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x16_aligna2_alignc2_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x16_aligna2_alignc8_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x16_aligna4_alignc8_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x16_aligna2_alignc4_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x16_aligna8_alignc4_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x16_alignc2_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x16_alignc8_execute_kernel_cudnn
sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel_cudnn
sm80_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize160x128x32_stage4_warpsize2x2x1_tensor16x8x16_kernel
sm80_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize192x128x32_stage3_warpsize4x2x1_tensor16x8x16_kernel
sm80_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x16_kernel
sm80_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize96x128x32_stage4_warpsize2x2x1_tensor16x8x16_kernel
sm80_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize96x64x32_stage4_warpsize2x2x1_tensor16x8x16_kernel
sm80_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize160x128x32_stage3_warpsize2x2x1_tensor16x8x16_kernel
sm80_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize192x128x32_stage3_warpsize4x2x1_tensor16x8x16_kernel
sm80_xmma_gemm_f16f16_f16f32_f32_nt_n_tilesize96x128x32_stage4_warpsize2x2x1_tensor16x8x16_kernel
sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize128x128x32_stage4_warpsize2x2x1_tensor16x8x16_kernel
sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x16_kernel
sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize96x128x32_stage4_warpsize2x2x1_tensor16x8x16_kernel
sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize96x64x32_stage4_warpsize2x2x1_tensor16x8x16_kernel
sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_execute_split_k_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x16_execute_split_k_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x16_execute_split_k_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize32x256x32_stage3_warpsize1x4x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r1s1_execute_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_execute_split_k_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_execute_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize16x64x64_stage1_warpsize1x4x1_g1_tensor16x8x16_aligna2_alignc2_execute_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize16x64x64_stage1_warpsize1x4x1_g1_tensor16x8x16_aligna4_alignc4_execute_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize32x16x64_stage1_warpsize2x1x1_g1_tensor16x8x16_aligna2_alignc2_execute_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize32x32x64_stage1_warpsize2x2x1_g1_tensor16x8x16_aligna2_alignc2_execute_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize32x32x64_stage1_warpsize2x2x1_g1_tensor16x8x16_aligna4_alignc2_execute_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x16x64_stage1_warpsize2x1x1_g1_tensor16x8x16_aligna2_alignc8_execute_kernel_cudnn
sm80_xmma_wgrad_implicit_gemm_indexed_wo_smem_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize16x64x64_stage1_warpsize1x4x1_g1_tensor16x8x8_aligna8_alignc8_execute_kernel_cudnn
std::enable_if<!T7, void>::type internal::gemvx::kernel<int, int, __half, __half, __half, float, (bool)0, (bool)0, (bool)0, (bool)0, (int)9, (bool)0, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float>>(T13)
std::enable_if<!T7, void>::type internal::gemvx::kernel<int, int, __half, __half, __half, float, (bool)0, (bool)1, (bool)0, (bool)0, (int)7, (bool)0, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float>>(T13)
std::enable_if<!T7, void>::type internal::gemvx::kernel<int, int, __half, __half, __half, float, (bool)0, (bool)1, (bool)1, (bool)0, (int)7, (bool)0, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float>>(T13)
std::enable_if<!T7, void>::type internal::gemvx::kernel<int, int, __half, float, float, float, (bool)0, (bool)1, (bool)0, (bool)0, (int)8, (bool)0, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>, float>>(T13)
std::enable_if<T7, void>::type internal::gemvx::kernel<int, int, __half, __half, __half, float, (bool)1, (bool)1, (bool)0, (bool)0, (int)5, (bool)0, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float>>(T13)
void convolve_common_engine_float_NHWC<__half, __half, (int)1024, (int)5, (int)5, (int)3, (int)3, (int)3, (bool)1, (bool)0, (bool)0, (bool)0, (bool)0>(int, int, int, const T1 *, const T1 *, int, T2 *, conv_kernel_common_params, unsigned long long, unsigned long, float, float, bool, const T2 *, const T2 *, bool)
void convolve_common_engine_float_NHWC<__half, __half, (int)1024, (int)6, (int)7, (int)3, (int)3, (int)5, (bool)1, (bool)0, (bool)0, (bool)0, (bool)0>(int, int, int, const T1 *, const T1 *, int, T2 *, conv_kernel_common_params, unsigned long long, unsigned long, float, float, bool, const T2 *, const T2 *, bool)
void convolve_common_engine_float_NHWC<__half, __half, (int)128, (int)5, (int)5, (int)3, (int)3, (int)3, (bool)1, (bool)0, (bool)0, (bool)0, (bool)0>(int, int, int, const T1 *, const T1 *, int, T2 *, conv_kernel_common_params, unsigned long long, unsigned long, float, float, bool, const T2 *, const T2 *, bool)
void convolve_common_engine_float_NHWC<__half, __half, (int)128, (int)6, (int)7, (int)3, (int)3, (int)5, (bool)1, (bool)0, (bool)0, (bool)0, (bool)0>(int, int, int, const T1 *, const T1 *, int, T2 *, conv_kernel_common_params, unsigned long long, unsigned long, float, float, bool, const T2 *, const T2 *, bool)
void convolve_common_engine_float_NHWC<__half, __half, (int)512, (int)6, (int)8, (int)3, (int)3, (int)5, (bool)1, (bool)0, (bool)0, (bool)0, (bool)0>(int, int, int, const T1 *, const T1 *, int, T2 *, conv_kernel_common_params, unsigned long long, unsigned long, float, float, bool, const T2 *, const T2 *, bool)
void cudnn::cnn::conv2d_grouped_direct_kernel<(bool)0, (bool)1, (bool)0, (bool)0, (int)0, (int)0, int, float, __half, __half, __half, float, __half>(cudnn::cnn::GroupedDirectFpropParams, const T9 *, const T11 *, T10 *, T12, T12, const T10 *, const T13 *, cudnnActivationStruct)
void cudnn::cnn::conv2d_grouped_direct_kernel<(bool)0, (bool)1, (bool)0, (bool)1, (int)0, (int)0, int, float, __half, __half, __half, float, __half>(cudnn::cnn::GroupedDirectFpropParams, const T9 *, const T11 *, T10 *, T12, T12, const T10 *, const T13 *, cudnnActivationStruct)
void cudnn::cnn::dgrad2d_grouped_direct_kernel<__half, int, float, float, (bool)0, (bool)1, (int)0, (int)0>(cudnn::cnn::GroupedDirectParams, const T1 *, const T1 *, T1 *, T4, T4)
void cudnn::cnn::dgrad2d_grouped_direct_kernel<__half, int, float, float, (bool)0, (bool)1, (int)0, (int)1>(cudnn::cnn::GroupedDirectParams, const T1 *, const T1 *, T1 *, T4, T4)
void cudnn::cnn::wgrad_alg1_engine_NHWC<__half, float, (int)128, (int)5, (int)5, (int)3, (int)3, (int)3, (bool)0, (bool)1>(int, int, int, const T1 *, int, T2 *, const T1 *, kernel_grad_params, unsigned long long, int, float, float, int, int, int *, int *, int, int)
void cudnn::cnn::wgrad_alg1_engine_NHWC<__half, float, (int)128, (int)6, (int)7, (int)3, (int)3, (int)5, (bool)0, (bool)1>(int, int, int, const T1 *, int, T2 *, const T1 *, kernel_grad_params, unsigned long long, int, float, float, int, int, int *, int *, int, int)
void cudnn::cnn::wgrad_alg1_engine_NHWC<__half, float, (int)512, (int)6, (int)5, (int)3, (int)3, (int)3, (bool)0, (bool)1>(int, int, int, const T1 *, int, T2 *, const T1 *, kernel_grad_params, unsigned long long, int, float, float, int, int, int *, int *, int, int)
void cudnn::cnn::wgrad_alg1_engine<__half, float, (int)128, (int)5, (int)5, (int)3, (int)3, (int)3, (bool)0, (bool)1>(int, int, int, const T1 *, int, T2 *, const T1 *, kernel_grad_params, unsigned long long, int, float, float, int, int, int *, int *, int, int)
void cudnn::cnn::wgrad_alg1_engine<__half, float, (int)128, (int)6, (int)7, (int)3, (int)3, (int)5, (bool)0, (bool)1>(int, int, int, const T1 *, int, T2 *, const T1 *, kernel_grad_params, unsigned long long, int, float, float, int, int, int *, int *, int, int)
void cudnn::cnn::wgrad_alg1_engine<__half, float, (int)512, (int)6, (int)5, (int)3, (int)3, (int)3, (bool)0, (bool)1>(int, int, int, const T1 *, int, T2 *, const T1 *, kernel_grad_params, unsigned long long, int, float, float, int, int, int *, int *, int, int)
void cudnn::cnn::wgrad2d_grouped_direct_kernel<(bool)0, (bool)1, int, __half, float, float>(cudnn::cnn::WgradGroupedDirectParams, const T4 *, const T4 *, T4 *, T6, T6)
void cudnn::detail::bn_bw_1C11_kernel_new<__half, float, float2, (int)512, (bool)1, (int)1>(T2, T2, T2, T2, cudnnTensorStruct, const T1 *, cudnnTensorStruct, const T1 *, cudnnTensorStruct, T1 *, const T2 *, T2 *, T2 *, const T2 *, const T2 *, T2)
void cudnn::detail::dgrad_alg1_engine<__half, (int)128, (int)5, (int)5, (int)3, (int)3, (int)3, (bool)0, (bool)0>(int, int, int, const T1 *, int, const T1 *, int, T1 *, kernel_grad_params, unsigned long long, int, float, int)
void cudnn::detail::dgrad_alg1_engine<__half, (int)128, (int)6, (int)7, (int)3, (int)3, (int)5, (bool)0, (bool)0>(int, int, int, const T1 *, int, const T1 *, int, T1 *, kernel_grad_params, unsigned long long, int, float, int)
void cudnn::detail::dgrad_alg1_engine<__half, (int)512, (int)6, (int)5, (int)3, (int)3, (int)3, (bool)0, (bool)0>(int, int, int, const T1 *, int, const T1 *, int, T1 *, kernel_grad_params, unsigned long long, int, float, int)
void cudnn::detail::dgrad_engine<__half, (int)128, (int)6, (int)7, (int)3, (int)3, (int)5, (bool)0>(int, int, int, const T1 *, int, const T1 *, int, T1 *, kernel_grad_params, unsigned long long, int, unsigned long long, int, float, int, int, int)
void cudnn::detail::dgrad_engine<__half, (int)128, (int)6, (int)8, (int)3, (int)3, (int)5, (bool)0>(int, int, int, const T1 *, int, const T1 *, int, T1 *, kernel_grad_params, unsigned long long, int, unsigned long long, int, float, int, int, int)
void cudnn::detail::dgrad_engine<__half, (int)128, (int)6, (int)8, (int)3, (int)3, (int)5, (bool)1>(int, int, int, const T1 *, int, const T1 *, int, T1 *, kernel_grad_params, int, int, float, int, int, int)
void cudnn::detail::dgrad_engine<__half, (int)512, (int)6, (int)5, (int)3, (int)3, (int)3, (bool)0>(int, int, int, const T1 *, int, const T1 *, int, T1 *, kernel_grad_params, unsigned long long, int, unsigned long long, int, float, int, int, int)
void cudnn::detail::dgrad2d_alg1_1<__half, (int)0, (int)4, (int)6, (int)3, (int)2, (int)4, (bool)0, (bool)1>(int, int, int, const T1 *, int, const T1 *, int, T1 *, kernel_grad_params, unsigned long long, int, unsigned long long, int, float, int, int)
void cudnn::detail::dgrad2d_alg1_1<__half, (int)0, (int)5, (int)6, (int)4, (int)3, (int)4, (bool)0, (bool)1>(int, int, int, const T1 *, int, const T1 *, int, T1 *, kernel_grad_params, unsigned long long, int, unsigned long long, int, float, int, int)
void cudnn::detail::dgrad2d_alg1_1<__half, (int)0, (int)6, (int)7, (int)5, (int)4, (int)5, (bool)0, (bool)1>(int, int, int, const T1 *, int, const T1 *, int, T1 *, kernel_grad_params, unsigned long long, int, unsigned long long, int, float, int, int)
void cudnn::detail::wgrad_alg0_engine<__half, (int)128, (int)6, (int)7, (int)3, (int)3, (int)5, (bool)1, (int)512>(int, int, int, const T1 *, int, T1 *, const T1 *, kernel_grad_params, int, float, int, int, int, int)
void cudnn::detail::wgrad_alg0_engine<__half, (int)128, (int)6, (int)8, (int)3, (int)3, (int)5, (bool)1, (int)512>(int, int, int, const T1 *, int, T1 *, const T1 *, kernel_grad_params, int, float, int, int, int, int)
void cudnn::detail::wgrad_alg1_engine<__half, (int)128, (int)6, (int)8, (int)3, (int)3, (int)5, (bool)1, (bool)1>(int, int, int, const T1 *, int, float *, const T1 *, kernel_grad_params, int, float, float, int, int, int *, int *, int, int)
void cudnn::ops::nchwToNhwcKernel<__half, __half, float, (bool)0, (bool)1, (cudnnKernelDataType_t)0>(cudnn::ops::nchw2nhwc_params_t<T3>, const T1 *, T2 *)
void cudnn::ops::nchwToNhwcKernel<__half, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::ops::nchw2nhwc_params_t<T3>, const T1 *, T2 *)
void cudnn::ops::nchwToNhwcKernel<float, __half, float, (bool)0, (bool)1, (cudnnKernelDataType_t)0>(cudnn::ops::nchw2nhwc_params_t<T3>, const T1 *, T2 *)
void cudnn::ops::nhwcToNchwKernel<__half, __half, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::ops::nhwc2nchw_params_t<T3>, const T1 *, T2 *)
void cudnn::ops::nhwcToNchwKernel<float, __half, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::ops::nhwc2nchw_params_t<T3>, const T1 *, T2 *)
void cudnn::winograd_nonfused::winogradForwardData4x4<float, __half>(cudnn::winograd_nonfused::WinogradDataParams<T1, T2>)
void cudnn::winograd_nonfused::winogradForwardFilter4x4<float, __half>(cudnn::winograd_nonfused::WinogradFilterParams<T1, T2>)
void cudnn::winograd_nonfused::winogradForwardOutput4x4<float, __half>(cudnn::winograd_nonfused::WinogradOutputParams<T1, T2>)
void cudnn::winograd_nonfused::winogradWgradData4x4<float, __half>(cudnn::winograd_nonfused::WinogradDataParams<T1, T2>)
void cudnn::winograd_nonfused::winogradWgradDelta4x4<float, __half>(cudnn::winograd_nonfused::WinogradDeltaParams<T1, T2>)
void cudnn::winograd_nonfused::winogradWgradOutput4x4<float, __half>(cudnn::winograd_nonfused::WinogradWgradOutputParams<T1, T2>)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmMultistage<cutlass_cudnn::gemm::GemmShape<(int)128, (int)128, (int)32>, cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)128, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)128, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)32, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)32, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)128, (int)128, (int)32>, cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::Array<float, (int)4, (bool)1>, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmMultistage<cutlass_cudnn::gemm::GemmShape<(int)128, (int)128, (int)32>, cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)128, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)128, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)32, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)32, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, (int)4, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)128, (int)128, (int)32>, cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::Array<float, (int)4, (bool)1>, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmMultistage<cutlass_cudnn::gemm::GemmShape<(int)128, (int)128, (int)32>, cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorOptimized<cutlass_cudnn::MatrixShape<(int)128, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass_cudnn::AlignedArray<cutlass_cudnn::half_t, (int)8, (int)16>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)128, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorOptimized<cutlass_cudnn::MatrixShape<(int)32, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass_cudnn::AlignedArray<cutlass_cudnn::half_t, (int)8, (int)16>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)32, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)128, (int)128, (int)32>, cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::Array<float, (int)4, (bool)1>, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmMultistage<cutlass_cudnn::gemm::GemmShape<(int)128, (int)256, (int)64>, cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)128, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)64>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)128, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)64>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)64, (int)256>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)256, (int)64>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)64, (int)256>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)256, (int)64>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)128, (int)256, (int)64>, cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)256, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)256, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::Array<float, (int)4, (bool)1>, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)256, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)256, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmMultistage<cutlass_cudnn::gemm::GemmShape<(int)128, (int)64, (int)32>, cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)128, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)128, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)32, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, (int)6, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)128, (int)64, (int)32>, cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)32, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)32, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::Array<float, (int)4, (bool)1>, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)32, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmMultistage<cutlass_cudnn::gemm::GemmShape<(int)128, (int)64, (int)64>, cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorOptimized<cutlass_cudnn::MatrixShape<(int)128, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)64>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass_cudnn::AlignedArray<cutlass_cudnn::half_t, (int)8, (int)16>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)128, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)64>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorOptimized<cutlass_cudnn::MatrixShape<(int)64, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)64>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass_cudnn::AlignedArray<cutlass_cudnn::half_t, (int)8, (int)16>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)64, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)64>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)128, (int)64, (int)64>, cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)32, (int)64>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::Array<float, (int)4, (bool)1>, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)32, (int)64>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmMultistage<cutlass_cudnn::gemm::GemmShape<(int)256, (int)128, (int)64>, cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorOptimized<cutlass_cudnn::MatrixShape<(int)256, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)256, (int)64>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass_cudnn::AlignedArray<cutlass_cudnn::half_t, (int)8, (int)16>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)256, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)256, (int)64>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorOptimized<cutlass_cudnn::MatrixShape<(int)64, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)64>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass_cudnn::AlignedArray<cutlass_cudnn::half_t, (int)8, (int)16>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)64, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)64>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, (int)3, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)256, (int)128, (int)64>, cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)4, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)256, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::Array<float, (int)4, (bool)1>, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)4, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)256, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmMultistage<cutlass_cudnn::gemm::GemmShape<(int)256, (int)64, (int)32>, cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)256, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)256, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)256, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)256, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, (int)4, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)256, (int)64, (int)32>, cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)8, (int)4, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::Array<float, (int)4, (bool)1>, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)8, (int)4, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmMultistage<cutlass_cudnn::gemm::GemmShape<(int)64, (int)128, (int)32>, cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)32, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)32, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, (int)6, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)64, (int)128, (int)32>, cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)4, (int)1, (int)1, (int)4>, (int)128, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::Array<float, (int)4, (bool)1>, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)4, (int)1, (int)1, (int)4>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmMultistage<cutlass_cudnn::gemm::GemmShape<(int)64, (int)256, (int)32>, cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)32, (int)256>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)256, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)32, (int)256>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)256, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, (int)4, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)64, (int)256, (int)32>, cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)256, (int)8, (int)1, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::Array<float, (int)4, (bool)1>, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)256, (int)8, (int)1, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)8, (int)1, (int)1, (int)8>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmMultistage<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, (int)10, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)4, (int)1, (int)1, (int)4>, (int)128, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::Array<float, (int)4, (bool)1>, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)4, (int)1, (int)1, (int)4>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmMultistage<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)64, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)64>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)64, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)64>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)64, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)64>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)64, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)64>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, (int)5, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)4, (int)1, (int)1, (int)4>, (int)128, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)64>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::Array<float, (int)4, (bool)1>, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)64>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)4, (int)1, (int)1, (int)4>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmMultistage<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorOptimized<cutlass_cudnn::MatrixShape<(int)64, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)64>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass_cudnn::AlignedArray<cutlass_cudnn::half_t, (int)8, (int)16>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)64, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)64>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorOptimized<cutlass_cudnn::MatrixShape<(int)64, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)64>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, cutlass_cudnn::AlignedArray<cutlass_cudnn::half_t, (int)8, (int)16>>, cutlass_cudnn::transform::threadblock::RegularTileAccessIterator<cutlass_cudnn::MatrixShape<(int)64, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)64>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, (cutlass_cudnn::arch::CacheOperation::Kind)0, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, (int)5, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)64>, cutlass_cudnn::gemm::warp::MmaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorTensorOpMultiplicandCongruous<(int)16, (int)64>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, (int)1, (bool)0, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)4, (int)1, (int)1, (int)4>, (int)128, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)64>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::Array<float, (int)4, (bool)1>, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)64>, cutlass_cudnn::gemm::GemmShape<(int)16, (int)8, (int)16>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)8, (int)2, (int)1, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)4, (int)1, (int)1, (int)4>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)16>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)8>, (int)2, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmPipelined<cutlass_cudnn::gemm::GemmShape<(int)128, (int)128, (int)32>, cutlass_cudnn::conv::threadblock::TileIterator<cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)128, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>>, cutlass_cudnn::transform::threadblock::RegularTileIterator<cutlass_cudnn::MatrixShape<(int)128, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, cutlass_cudnn::conv::threadblock::TileIterator<cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)32, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>>, cutlass_cudnn::transform::threadblock::RegularTileIterator<cutlass_cudnn::MatrixShape<(int)32, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::TensorNHWC, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)16, (int)4>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, cutlass_cudnn::NumericArrayConverter<cutlass_cudnn::half_t, cutlass_cudnn::half_t, (int)32, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::NumericArrayConverter<cutlass_cudnn::half_t, cutlass_cudnn::half_t, (int)32, (cutlass_cudnn::FloatRoundStyle)2>, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)128, (int)128, (int)32>, cutlass_cudnn::gemm::warp::MmaVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)16, (int)4>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)4, (int)4, (int)2, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)2, (int)2, (int)1, (int)4>, (int)128, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)4>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)4>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)4, (int)4, (int)2, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)2, (int)2, (int)1, (int)4>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)8>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)2>, (int)1, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmPipelined<cutlass_cudnn::gemm::GemmShape<(int)128, (int)256, (int)32>, cutlass_cudnn::conv::threadblock::TileIterator<cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)128, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>>, cutlass_cudnn::transform::threadblock::RegularTileIterator<cutlass_cudnn::MatrixShape<(int)128, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, cutlass_cudnn::conv::threadblock::TileIterator<cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)32, (int)256>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)256, (int)32>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>>, cutlass_cudnn::transform::threadblock::RegularTileIterator<cutlass_cudnn::MatrixShape<(int)32, (int)256>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)256, (int)32>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::TensorNHWC, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)16, (int)4>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, cutlass_cudnn::NumericArrayConverter<cutlass_cudnn::half_t, cutlass_cudnn::half_t, (int)16, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::NumericArrayConverter<cutlass_cudnn::half_t, cutlass_cudnn::half_t, (int)32, (cutlass_cudnn::FloatRoundStyle)2>, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)128, (int)256, (int)32>, cutlass_cudnn::gemm::warp::MmaVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)16, (int)4>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)256, (int)4, (int)4, (int)2, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)2, (int)2, (int)1, (int)4>, (int)256, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)4>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)4>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)256, (int)4, (int)4, (int)2, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)2, (int)2, (int)1, (int)4>, (int)256, (int)4, (int)32>::CompactedThreadMap, float, (int)8>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)2>, (int)1, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmPipelined<cutlass_cudnn::gemm::GemmShape<(int)256, (int)128, (int)32>, cutlass_cudnn::conv::threadblock::TileIterator<cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)256, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)256, (int)32>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>>, cutlass_cudnn::transform::threadblock::RegularTileIterator<cutlass_cudnn::MatrixShape<(int)256, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)256, (int)32>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, cutlass_cudnn::conv::threadblock::TileIterator<cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)32, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>>, cutlass_cudnn::transform::threadblock::RegularTileIterator<cutlass_cudnn::MatrixShape<(int)32, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)256, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::TensorNHWC, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)16, (int)4>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, cutlass_cudnn::NumericArrayConverter<cutlass_cudnn::half_t, cutlass_cudnn::half_t, (int)32, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::NumericArrayConverter<cutlass_cudnn::half_t, cutlass_cudnn::half_t, (int)16, (cutlass_cudnn::FloatRoundStyle)2>, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)256, (int)128, (int)32>, cutlass_cudnn::gemm::warp::MmaVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)16, (int)4>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)4, (int)4, (int)4, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)2, (int)2, (int)1, (int)4>, (int)256, (int)4, (int)32>, float>, cutlass_cudnn::epilogue::warp::FragmentIteratorVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)4>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)4>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)4, (int)4, (int)4, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)2, (int)2, (int)1, (int)4>, (int)256, (int)4, (int)32>::CompactedThreadMap, float, (int)8>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)2>, (int)1, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmPipelined<cutlass_cudnn::gemm::GemmShape<(int)64, (int)128, (int)32>, cutlass_cudnn::conv::threadblock::TileIterator<cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>>, cutlass_cudnn::transform::threadblock::RegularTileIterator<cutlass_cudnn::MatrixShape<(int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, cutlass_cudnn::conv::threadblock::TileIterator<cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)32, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>>, cutlass_cudnn::transform::threadblock::RegularTileIterator<cutlass_cudnn::MatrixShape<(int)32, (int)128>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)128, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::TensorNHWC, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)16, (int)4>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, cutlass_cudnn::NumericArrayConverter<cutlass_cudnn::half_t, cutlass_cudnn::half_t, (int)16, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::NumericArrayConverter<cutlass_cudnn::half_t, cutlass_cudnn::half_t, (int)32, (cutlass_cudnn::FloatRoundStyle)2>, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)64, (int)128, (int)32>, cutlass_cudnn::gemm::warp::MmaVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)16, (int)4>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)4, (int)4, (int)2, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)2, (int)1, (int)1, (int)2>, (int)128, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)4>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)64, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)4>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)128, (int)4, (int)4, (int)2, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)2, (int)1, (int)1, (int)2>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)8>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)2>, (int)1, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmPipelined<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::conv::threadblock::TileIterator<cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>>, cutlass_cudnn::transform::threadblock::RegularTileIterator<cutlass_cudnn::MatrixShape<(int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, cutlass_cudnn::conv::threadblock::TileIterator<cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>>, cutlass_cudnn::transform::threadblock::RegularTileIterator<cutlass_cudnn::MatrixShape<(int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::TensorNHWC, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)16, (int)4>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, cutlass_cudnn::NumericArrayConverter<cutlass_cudnn::half_t, cutlass_cudnn::half_t, (int)16, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::NumericArrayConverter<cutlass_cudnn::half_t, cutlass_cudnn::half_t, (int)16, (cutlass_cudnn::FloatRoundStyle)2>, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::warp::MmaVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)16, (int)4>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)4, (int)4, (int)2, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)2, (int)1, (int)1, (int)2>, (int)128, (int)4, (int)32>, float, (bool)0>, cutlass_cudnn::epilogue::warp::FragmentIteratorVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)4>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)4>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)4, (int)4, (int)2, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)2, (int)1, (int)1, (int)2>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)8>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)2>, (int)1, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass_cudnn::conv::threadblock::ImplicitGemmPipelined<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::conv::threadblock::TileIterator<cutlass_cudnn::conv::threadblock::Conv2dWgradOutputGradientTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>>, cutlass_cudnn::transform::threadblock::RegularTileIterator<cutlass_cudnn::MatrixShape<(int)64, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, (int)1, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, cutlass_cudnn::conv::threadblock::TileIterator<cutlass_cudnn::conv::threadblock::Conv2dWgradActivationTileAccessIteratorAnalytic<cutlass_cudnn::MatrixShape<(int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>>>, cutlass_cudnn::transform::threadblock::RegularTileIterator<cutlass_cudnn::MatrixShape<(int)32, (int)64>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, (int)0, cutlass_cudnn::transform::PitchLinearWarpRakedThreadMap<cutlass_cudnn::PitchLinearShape<(int)64, (int)32>, (int)128, cutlass_cudnn::PitchLinearShape<(int)8, (int)4>, (int)8>, (int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::TensorNHWC, cutlass_cudnn::gemm::threadblock::MmaPolicy<cutlass_cudnn::gemm::warp::MmaVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)16, (int)4>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, bool>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, cutlass_cudnn::MatrixShape<(int)0, (int)0>, (int)1>, cutlass_cudnn::NumericArrayConverter<cutlass_cudnn::half_t, cutlass_cudnn::half_t, (int)16, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::NumericArrayConverter<cutlass_cudnn::half_t, cutlass_cudnn::half_t, (int)16, (cutlass_cudnn::FloatRoundStyle)2>, bool>, cutlass_cudnn::epilogue::threadblock::Epilogue<cutlass_cudnn::gemm::GemmShape<(int)64, (int)64, (int)32>, cutlass_cudnn::gemm::warp::MmaVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)32>, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<(int)16>, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<(int)16>, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::gemm::warp::MmaTensorOpPolicy<cutlass_cudnn::arch::Mma<cutlass_cudnn::gemm::GemmShape<(int)16, (int)16, (int)4>, (int)32, cutlass_cudnn::half_t, cutlass_cudnn::layout::ColumnMajor, cutlass_cudnn::half_t, cutlass_cudnn::layout::RowMajor, float, cutlass_cudnn::layout::RowMajor, cutlass_cudnn::arch::OpMultiplyAdd>, cutlass_cudnn::MatrixShape<(int)1, (int)1>>, bool>, (int)1, cutlass_cudnn::epilogue::threadblock::PredicatedTileIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)4, (int)4, (int)2, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)2, (int)1, (int)1, (int)2>, (int)128, (int)4, (int)32>, float>, cutlass_cudnn::epilogue::warp::FragmentIteratorVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)4>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::warp::TileIteratorVoltaTensorOp<cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)32>, cutlass_cudnn::gemm::GemmShape<(int)32, (int)32, (int)4>, float, cutlass_cudnn::layout::RowMajor>, cutlass_cudnn::epilogue::threadblock::SharedLoadIterator<cutlass_cudnn::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)64, (int)4, (int)4, (int)2, (int)1>, cutlass_cudnn::epilogue::threadblock::OutputTileShape<(int)1, (int)2, (int)1, (int)1, (int)2>, (int)128, (int)4, (int)32>::CompactedThreadMap, float, (int)8>, cutlass_cudnn::epilogue::thread::LinearCombination<float, (int)4, float, float, (cutlass_cudnn::epilogue::thread::ScaleType::Kind)0, (cutlass_cudnn::FloatRoundStyle)2>, cutlass_cudnn::MatrixShape<(int)0, (int)2>, (int)1, (int)1>, cutlass_cudnn::gemm::threadblock::GemmIdentityThreadblockSwizzle<(int)4>, (cutlass_cudnn::conv::Operator)2, cutlass_cudnn::conv::Conv2dProblemSize>>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_analytic_f16_256x128_32x3_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_analytic_f16_64x256_32x4_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_128x128_32x4_nhwc_align8>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_128x128_32x4_nhwc_unity_stride_align8>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_128x128_32x4_nhwc_unity_stride>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_128x128_32x4_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_128x64_32x6_nhwc_unity_stride>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_256x64_32x4_nhwc_unity_stride>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_256x64_32x4_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_64x128_32x6_nhwc_unity_stride>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_64x128_64x3_nhwc_align8>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_64x128_64x3_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_64x256_32x4_nhwc_align8>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_64x256_32x4_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_64x256_64x4_nhwc_align8>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_64x256_64x4_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_64x64_64x5_nhwc_unity_stride>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_64x64_64x5_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_analytic_f16_128x128_32x3_nhwc_align8>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_optimized_f16_128x128_32x3_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_optimized_f16_128x128_32x4_nhwc_align8>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_optimized_f16_128x64_32x6_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_optimized_f16_256x128_32x3_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_optimized_f16_256x64_32x4_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_optimized_f16_64x128_32x6_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_optimized_f16_64x128_64x3_nhwc_align8>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_optimized_f16_64x256_32x4_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_optimized_f16_64x64_32x10_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_optimized_f16_64x64_64x5_nhwc_align8>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816wgrad_optimized_f16_256x128_32x3_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816wgrad_optimized_f16_256x64_32x4_nhwc_align8>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816wgrad_optimized_f16_64x128_32x6_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s16816wgrad_optimized_f16_64x64_32x10_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s884fprop_analytic_f16_64x64_32x2_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s884wgrad_analytic_f16_256x128_32x2_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s884wgrad_analytic_f16_64x128_32x2_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_f16_s884wgrad_analytic_f16_64x64_32x2_nhwc>(T1::Params)
void cutlass_cudnn::Kernel<cutlass_tensorop_s1688fprop_optimized_tf32_64x64_16x10_nhwc>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_f16_128x128_nn_align1>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_f16_128x256_tn_align1>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_f16_128x64_nt_align1>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_f16_128x64_tn_align1>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_f16_256x128_nn_align1>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_f16_256x128_nt_align1>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_f16_64x128_nt_align2>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_f16_64x64_nn_align1>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_f16_64x64_nn_align2>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_f16_64x64_nt_align1>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_f16_64x64_nt_align2>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_f16_64x64_tn_align1>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_f16_64x64_tn_align2>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_relu_f16_128x128_nn_align8>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_relu_f16_128x128_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_relu_f16_128x128_tn_align8>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_relu_f16_128x256_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_relu_f16_128x64_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_relu_f16_256x128_nn_align8>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_relu_f16_256x128_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_relu_f16_64x64_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_70_tensorop_f16_s884gemm_relu_f16_64x64_tn_align8>(T1::Params)
void cutlass::Kernel<cutlass_70_wmma_tensorop_f16_s161616gemm_f16_16x16_64x2_nn_align8>(T1::Params)
void cutlass::Kernel<cutlass_70_wmma_tensorop_f16_s161616gemm_f16_16x16_64x2_tn_align8>(T1::Params)
void cutlass::Kernel<cutlass_70_wmma_tensorop_f16_s161616gemm_f16_32x32_64x2_nn_align1>(T1::Params)
void cutlass::Kernel<cutlass_70_wmma_tensorop_f16_s161616gemm_f16_32x32_64x2_nn_align2>(T1::Params)
void cutlass::Kernel<cutlass_70_wmma_tensorop_f16_s161616gemm_f16_32x32_64x2_nn_align8>(T1::Params)
void cutlass::Kernel<cutlass_70_wmma_tensorop_f16_s161616gemm_f16_32x32_64x2_nt_align1>(T1::Params)
void cutlass::Kernel<cutlass_70_wmma_tensorop_f16_s161616gemm_f16_32x32_64x2_nt_align2>(T1::Params)
void cutlass::Kernel<cutlass_70_wmma_tensorop_f16_s161616gemm_f16_32x32_64x2_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_70_wmma_tensorop_f16_s161616gemm_f16_32x32_64x2_tn_align2>(T1::Params)
void cutlass::Kernel<cutlass_70_wmma_tensorop_f16_s161616gemm_f16_32x32_64x2_tn_align8>(T1::Params)
void cutlass::Kernel<cutlass_70_wmma_tensorop_s161616gemm_f16_32x32_64x2_tn_align1>(T1::Params)
void cutlass::Kernel<cutlass_70_wmma_tensorop_s161616gemm_f16_32x32_64x2_tn_align8>(T1::Params)
void cutlass::Kernel<cutlass_75_tensorop_f16_s1688gemm_f16_128x128_nt_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_tensorop_f16_s1688gemm_f16_128x256_nn_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_tensorop_f16_s1688gemm_f16_128x256_tn_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_tensorop_f16_s1688gemm_f16_128x64_nn_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_tensorop_f16_s1688gemm_f16_128x64_nt_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_tensorop_f16_s1688gemm_f16_256x128_nn_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_tensorop_f16_s1688gemm_f16_64x64_nn_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_tensorop_f16_s1688gemm_f16_64x64_nt_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_tensorop_f16_s1688gemm_f16_64x64_tn_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s161616gemm_f16_32x32_128x1_nn_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s161616gemm_f16_32x32_128x2_nt_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s161616gemm_f16_32x32_128x2_tn_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s161616gemm_f16_32x32_64x1_nn_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s161616gemm_f16_32x32_64x1_tn_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s161616gemm_f16_32x32_64x2_nn_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_wmma_tensorop_s161616gemm_f16_16x16_64x2_nt_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_wmma_tensorop_s161616gemm_f16_32x32_64x1_nt_align1>(T1::Params)
void cutlass::Kernel<cutlass_75_wmma_tensorop_s161616gemm_f16_32x32_64x1_tn_align1>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_f16_128x128_32x4_nn_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_f16_128x256_32x3_tn_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_f16_128x64_32x6_nn_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_f16_64x64_64x4_nn_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_f16_64x64_64x4_nt_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x128_32x4_nn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x128_32x4_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x128_32x4_tn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x128_32x5_nn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x128_32x5_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x128_32x5_tn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x128_32x6_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x128_64x3_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_256x64_32x4_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_64x256_32x4_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_64x256_64x4_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_64x64_32x6_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_64x64_32x6_tn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_64x64_64x6_nn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_64x64_64x6_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_f16_s16816gemm_relu_f16_64x64_64x6_tn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_tensorop_s16816gemm_f16_64x64_32x6_nn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_16x16_64x1_nn_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_16x16_64x1_nn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_16x16_64x1_tn_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_16x16_64x1_tn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_16x16_64x2_tn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_128x1_nn_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_128x1_nn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_128x1_nt_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_128x1_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_128x1_tn_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_128x1_tn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_128x2_nn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_128x2_nt_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_128x2_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_128x2_tn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_32x1_nn_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_32x1_nn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_32x1_nt_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_32x1_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_32x1_tn_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_32x1_tn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_32x1_tt_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_64x1_nn_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_64x1_nn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_64x1_nt_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_64x1_nt_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_64x1_tn_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_64x1_tn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_s161616gemm_f16_32x32_128x1_nn_align8>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_s161616gemm_f16_32x32_128x2_nt_align2>(T1::Params)
void cutlass::Kernel<cutlass_80_wmma_tensorop_s161616gemm_f16_32x32_128x2_tn_align8>(T1::Params)
void explicit_convolve_sgemm<__half, int, (int)128, (int)5, (int)5, (int)3, (int)3, (int)3, (int)0, (bool)0>(int, int, int, const T1 *, int, const T1 *, int, T1 *, kernel_conv_params, unsigned long long, int, unsigned long long, int, float, float, int, bool, const T1 *, const T1 *)
void explicit_convolve_sgemm<__half, int, (int)128, (int)6, (int)7, (int)3, (int)3, (int)5, (int)0, (bool)0>(int, int, int, const T1 *, int, const T1 *, int, T1 *, kernel_conv_params, unsigned long long, int, unsigned long long, int, float, float, int, bool, const T1 *, const T1 *)
void explicit_convolve_sgemm<__half, int, (int)512, (int)6, (int)8, (int)3, (int)3, (int)5, (int)0, (bool)0>(int, int, int, const T1 *, int, const T1 *, int, T1 *, kernel_conv_params, unsigned long long, int, unsigned long long, int, float, float, int, bool, const T1 *, const T1 *)
void gemmk1_kernel<float, (int)256, (int)5, (bool)0, (bool)0, (bool)0, (bool)0, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float>(cublasGemmk1Params<T1, T8, T9, T10>)
void gemmk1_kernel<float, (int)256, (int)5, (bool)0, (bool)0, (bool)1, (bool)0, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float>(cublasGemmk1Params<T1, T8, T9, T10>)
void gemmk1_kernel<int, float, (int)256, (int)5, (bool)0, (bool)0, (bool)0, (bool)0, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float, (int)0>(cublasGemmk1Params<T2, T9, T10, T11, T12, biasType<T11::value_type, T12>::type>)
void gemmk1_kernel<int, float, (int)256, (int)5, (bool)1, (bool)0, (bool)0, (bool)0, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float, (int)0>(cublasGemmk1Params<T2, T9, T10, T11, T12, biasType<T11::value_type, T12>::type>)
void gemmSN_NN_kernel<float, (int)128, (int)2, (int)4, (int)8, (int)2, (int)4, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T8, T9, T1>)
void gemmSN_NN_kernel<float, (int)128, (int)2, (int)4, (int)8, (int)3, (int)4, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T8, T9, T1>)
void gemmSN_NN_kernel<float, (int)128, (int)2, (int)4, (int)8, (int)4, (int)4, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T8, T9, T1>)
void gemmSN_NN_kernel<float, (int)128, (int)2, (int)4, (int)8, (int)5, (int)4, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T8, T9, T1>)
void gemmSN_NN_kernel<float, (int)128, (int)2, (int)4, (int)8, (int)6, (int)4, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T8, T9, T1>)
void gemmSN_NN_kernel<float, (int)128, (int)2, (int)4, (int)8, (int)7, (int)4, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T8, T9, T1>)
void gemmSN_NN_kernel<float, (int)256, (int)4, (int)2, (int)8, (int)3, (int)4, (bool)0, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T9, T10, T11, T1>)
void gemmSN_NN_kernel<float, (int)256, (int)4, (int)2, (int)8, (int)3, (int)4, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T8, T9, T1>)
void gemmSN_NN_kernel<float, (int)256, (int)4, (int)2, (int)8, (int)4, (int)4, (bool)0, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T9, T10, T11, T1>)
void gemmSN_NN_kernel<float, (int)256, (int)4, (int)2, (int)8, (int)4, (int)4, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T8, T9, T1>)
void gemmSN_NN_kernel<float, (int)256, (int)4, (int)2, (int)8, (int)5, (int)4, (bool)0, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T9, T10, T11, T1>)
void gemmSN_NN_kernel<float, (int)256, (int)4, (int)2, (int)8, (int)5, (int)4, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T8, T9, T1>)
void gemmSN_NN_kernel<float, (int)256, (int)4, (int)2, (int)8, (int)6, (int)4, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T8, T9, T1>)
void gemmSN_NN_kernel<float, (int)256, (int)4, (int)2, (int)8, (int)7, (int)4, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T8, T9, T1>)
void gemmSN_TN_kernel<float, (int)128, (int)16, (int)2, (int)4, (int)6, (int)7, (bool)0, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T9, T10, T11, T1>)
void gemmSN_TN_kernel<float, (int)128, (int)16, (int)2, (int)4, (int)8, (int)9, (bool)0, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<T9, T10, T11, T1>)
void gemv2N_kernel<int, int, __half, __half, float, (int)128, (int)32, (int)4, (int)4, (int)1, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float>>(T11)
void gemv2N_kernel<int, int, __half, __half, float, (int)128, (int)4, (int)4, (int)4, (int)1, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float>>(T11)
void gemv2N_kernel<int, int, __half, __half, float, (int)128, (int)4, (int)4, (int)4, (int)11, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float>>(T11)
void gemv2N_kernel<int, int, __half, __half, float, (int)128, (int)8, (int)4, (int)4, (int)1, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float>>(T11)
void gemv2N_kernel<int, int, __half, float, float, (int)128, (int)8, (int)4, (int)4, (int)1, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<float>, float>>(T11)
void gemv2N_kernel<int, int, __half, float, float, float, (int)128, (int)1, (int)2, (int)2, (int)1, (bool)0, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>, float>>(T13)
void gemv2N_kernel<int, int, __half, float, float, float, (int)128, (int)8, (int)4, (int)4, (int)1, (bool)0, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const float>, cublasGemvTensorStridedBatched<float>, float>>(T13)
void gemv2T_kernel_val<int, int, __half, __half, __half, float, (int)128, (int)16, (int)2, (int)4, (bool)0, (bool)0, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float>>(T13, T6, T6)
void gemv2T_kernel_val<int, int, __half, __half, __half, float, (int)128, (int)16, (int)4, (int)4, (bool)0, (bool)0, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float>>(T13, T6, T6)
void gemv2T_kernel_val<int, int, __half, __half, float, (int)128, (int)16, (int)2, (int)4, (bool)0, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float>>(T11, T5, T5)
void gemv2T_kernel_val<int, int, __half, float, float, (int)128, (int)16, (int)2, (int)2, (bool)0, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<float>, float>>(T11, T5, T5)
void gemvNSP_kernel<__half, __half, __half, float, (int)11, (int)32, (int)4, (int)1024, (bool)0, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float>>(T10)
void gemvNSP_kernel<__half, __half, float, (int)11, (int)32, (int)4, (int)1024, cublasGemvParams<cublasGemvTensorStridedBatched<const __half>, cublasGemvTensorStridedBatched<__half>, float>>(T8)
void implicit_convolve_sgemm<__half, __half, (int)1024, (int)5, (int)5, (int)3, (int)3, (int)3, (int)1, (bool)0, (bool)0, (bool)1>(int, int, int, const T1 *, int, T2 *, const T1 *, kernel_conv_params, unsigned long long, int, float, float, int, const T2 *, const T2 *, bool, int, int)
void implicit_convolve_sgemm<__half, __half, (int)128, (int)6, (int)7, (int)3, (int)3, (int)5, (int)1, (bool)0, (bool)0, (bool)1>(int, int, int, const T1 *, int, T2 *, const T1 *, kernel_conv_params, unsigned long long, int, float, float, int, const T2 *, const T2 *, bool, int, int)
void implicit_convolve_sgemm<__half, __half, (int)512, (int)6, (int)8, (int)3, (int)3, (int)5, (int)1, (bool)0, (bool)0, (bool)1>(int, int, int, const T1 *, int, T2 *, const T1 *, kernel_conv_params, unsigned long long, int, float, float, int, const T2 *, const T2 *, bool, int, int)
void nchwToNhwcKernel<__half, __half, float, (bool)1, (bool)0>(int, int, int, int, const T1 *, T2 *, T3, T3)
void nchwToNhwcKernel<__half, __half, float, (bool)1, (bool)1>(int, int, int, int, const T1 *, T2 *, T3, T3)
void nhwcToNchwKernel<float, __half, float, (bool)1, (bool)0>(int, int, int, int, const T1 *, T2 *, T3, T3)
void nhwcToNchwKernel<float, __half, float, (bool)1, (bool)1>(int, int, int, int, const T1 *, T2 *, T3, T3)
void pointwise_mult_and_sum_complex<float2, (int)8, (int)4>(T1 *, T1 *, T1 *, int, int, int, int, int, float2)
void precomputed_convolve_sgemm<__half, (int)1024, (int)5, (int)5, (int)4, (int)3, (int)3, (int)1, (bool)0>(int, int, int, const T1 *, int, T1 *, const T1 *, kernel_conv_params, unsigned long long, int, float, float, int, bool, const T1 *, const T1 *, int *)
void precomputed_convolve_sgemm<__half, (int)128, (int)5, (int)5, (int)3, (int)3, (int)3, (int)1, (bool)0>(int, int, int, const T1 *, int, T1 *, const T1 *, kernel_conv_params, unsigned long long, int, float, float, int, bool, const T1 *, const T1 *, int *)
void precomputed_convolve_sgemm<__half, (int)128, (int)6, (int)7, (int)3, (int)3, (int)5, (int)1, (bool)0>(int, int, int, const T1 *, int, T1 *, const T1 *, kernel_conv_params, unsigned long long, int, float, float, int, bool, const T1 *, const T1 *, int *)
void precomputed_convolve_sgemm<__half, (int)512, (int)6, (int)7, (int)4, (int)3, (int)5, (int)1, (bool)0>(int, int, int, const T1 *, int, T1 *, const T1 *, kernel_conv_params, unsigned long long, int, float, float, int, bool, const T1 *, const T1 *, int *)
void precomputed_convolve_sgemm<__half, (int)512, (int)6, (int)8, (int)3, (int)3, (int)5, (int)1, (bool)0>(int, int, int, const T1 *, int, T1 *, const T1 *, kernel_conv_params, unsigned long long, int, float, float, int, bool, const T1 *, const T1 *, int *)
void tensorflow::BiasGradNHWC_SharedAtomics<Eigen::half>(int, const T1 *, T1 *, int)
void tensorflow::BiasNHWCKernel<Eigen::half>(int, const T1 *, const T1 *, T1 *, int)
void wgrad_alg0_engine_NHWC<__half, (int)128, (int)5, (int)5, (int)3, (int)3, (int)3, (bool)0, (int)512>(int, int, int, const T1 *, int, T1 *, const T1 *, kernel_grad_params, unsigned long long, int, float, int, int, int, int)
void wgrad_alg0_engine_NHWC<__half, (int)128, (int)6, (int)7, (int)3, (int)3, (int)5, (bool)0, (int)512>(int, int, int, const T1 *, int, T1 *, const T1 *, kernel_grad_params, unsigned long long, int, float, int, int, int, int)
void wgrad_alg0_engine_NHWC<__half, (int)512, (int)6, (int)5, (int)3, (int)3, (int)3, (bool)0, (int)512>(int, int, int, const T1 *, int, T1 *, const T1 *, kernel_grad_params, unsigned long long, int, float, int, int, int, int)
void wgrad_alg0_engine<__half, (int)128, (int)5, (int)5, (int)3, (int)3, (int)3, (bool)0, (int)512>(int, int, int, const T1 *, int, T1 *, const T1 *, kernel_grad_params, unsigned long long, int, float, int, int, int, int)
void wgrad_alg0_engine<__half, (int)128, (int)6, (int)7, (int)3, (int)3, (int)5, (bool)0, (int)512>(int, int, int, const T1 *, int, T1 *, const T1 *, kernel_grad_params, unsigned long long, int, float, int, int, int, int)
void wgrad_alg0_engine<__half, (int)128, (int)6, (int)8, (int)3, (int)3, (int)5, (bool)0, (int)512>(int, int, int, const T1 *, int, T1 *, const T1 *, kernel_grad_params, unsigned long long, int, float, int, int, int, int)
void wgrad_alg0_engine<__half, (int)512, (int)6, (int)5, (int)3, (int)3, (int)3, (bool)0, (int)512>(int, int, int, const T1 *, int, T1 *, const T1 *, kernel_grad_params, unsigned long long, int, float, int, int, int, int)
void xmma_cudnn::ext::implicit_gemm::kernel<xmma_cudnn::ext::implicit_gemm::indexed_wo_smem::wgrad::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)16, (int)64, (int)64, (int)1, (int)4, (int)1, (int)1>, (int)2, (int)2>>(T1::Params)
void xmma_cudnn::gemm::kernel<xmma_cudnn::implicit_gemm::dgrad::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::dgrad::Gmem_tile_a_t<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::Input_related<(int)1, (int)3, (int)3, (bool)0>, (int)16, (bool)0, xmma_cudnn::implicit_gemm::dgrad::Gmem_tile_base_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::Input_related<(int)1, (int)3, (int)3, (bool)0>, (int)16, xmma_cudnn::Row, (int)32, (int)128>>, xmma_cudnn::implicit_gemm::dgrad::Gmem_tile_c_t<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, (int)16, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>, (bool)0>, xmma_cudnn::implicit_gemm::Input_related<(int)1, (int)3, (int)3, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::gemm::kernel<xmma_cudnn::implicit_gemm::dgrad::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::dgrad::Gmem_tile_a_t<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)16, (bool)0, xmma_cudnn::implicit_gemm::dgrad::Gmem_tile_base_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)16, xmma_cudnn::Row, (int)64, (int)64>>, xmma_cudnn::implicit_gemm::dgrad::Gmem_tile_c_t<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (int)16, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)0>, (bool)0>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::gemm::kernel<xmma_cudnn::implicit_gemm::dgrad::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::dgrad::Gmem_tile_a_t<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::Input_related<(int)1, (int)1, (int)1, (bool)1>, (int)16, (bool)0, xmma_cudnn::implicit_gemm::dgrad::Gmem_tile_base_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::Input_related<(int)1, (int)1, (int)1, (bool)1>, (int)16, xmma_cudnn::Row, (int)64, (int)64>>, xmma_cudnn::implicit_gemm::dgrad::Gmem_tile_c_t<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (int)16, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)0>, (bool)0>, xmma_cudnn::implicit_gemm::Input_related<(int)1, (int)1, (int)1, (bool)1>, (int)1>>(T1::Params)
void xmma_cudnn::gemm::kernel<xmma_cudnn::implicit_gemm::wgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_a_n<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (int)16, xmma_cudnn::Col, (int)128, (int)64>>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_b_t<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (bool)0, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_b<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (bool)0, (int)16, xmma_cudnn::Row, (int)128, (int)64>>, (bool)0, (int)1>>(T1::Params)
void xmma_cudnn::gemm::kernel<xmma_cudnn::implicit_gemm::wgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_a_n<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (int)16, xmma_cudnn::Col, (int)128, (int)64>>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_b_t<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (bool)1, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_b<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (bool)1, (int)16, xmma_cudnn::Row, (int)128, (int)64>>, (bool)1, (int)1>>(T1::Params)
void xmma_cudnn::gemm::kernel<xmma_cudnn::implicit_gemm::wgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_a_n<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (int)16, xmma_cudnn::Col, (int)128, (int)64>>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_b_t<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)1, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_b<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)1, (int)16, xmma_cudnn::Row, (int)64, (int)64>>, (bool)1, (int)1>>(T1::Params)
void xmma_cudnn::gemm::kernel<xmma_cudnn::implicit_gemm::wgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_a_n<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, (int)16, xmma_cudnn::Col, (int)64, (int)32>>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_b_t<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_b<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0, (int)16, xmma_cudnn::Row, (int)128, (int)32>>, (bool)0, (int)1>>(T1::Params)
void xmma_cudnn::gemm::kernel<xmma_cudnn::implicit_gemm::wgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_a_n<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)2, (int)1, (int)1>, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)2, (int)1, (int)1>, (int)16, xmma_cudnn::Col, (int)64, (int)64>>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_b_t<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)1, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_b<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)1, (int)16, xmma_cudnn::Row, (int)32, (int)64>>, (bool)1, (int)1>>(T1::Params)
void xmma_cudnn::gemm::kernel<xmma_cudnn::implicit_gemm::wgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_a_n<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (int)16, xmma_cudnn::Col, (int)64, (int)64>>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_b_t<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)0, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_b<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)0, (int)16, xmma_cudnn::Row, (int)64, (int)64>>, (bool)0, (int)1>>(T1::Params)
void xmma_cudnn::gemm::split_k_kernel<xmma_cudnn::implicit_gemm::wgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_a_n<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (int)16, xmma_cudnn::Col, (int)128, (int)64>>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_b_t<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (bool)0, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_b<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (bool)0, (int)16, xmma_cudnn::Row, (int)128, (int)64>>, (bool)0, (int)1>>(T1::Params)
void xmma_cudnn::gemm::split_k_kernel<xmma_cudnn::implicit_gemm::wgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_a_n<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (int)16, xmma_cudnn::Col, (int)128, (int)64>>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_b_t<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (bool)1, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_b<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)128, (int)64, (int)2, (int)2, (int)2, (int)1>, (bool)1, (int)16, xmma_cudnn::Row, (int)128, (int)64>>, (bool)1, (int)1>>(T1::Params)
void xmma_cudnn::gemm::split_k_kernel<xmma_cudnn::implicit_gemm::wgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_a_n<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (int)16, xmma_cudnn::Col, (int)128, (int)64>>, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_b_t<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)1, (int)16, (bool)0, xmma_cudnn::implicit_gemm::wgrad_indexed::Gmem_tile_base_b<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)1, (int)16, xmma_cudnn::Row, (int)64, (int)64>>, (bool)1, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_0<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)4>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_0<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)4>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_0<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)3>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_0<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)4>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_0<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_0<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_0<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_0<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_0<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>, (bool)1>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_0<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_1<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)4>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_1<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)4>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_1<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)3>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_1<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)4>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_1<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_1<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_1<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_1<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_1<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>, (bool)1>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_1<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_2<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)4>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_2<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)4>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_2<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)3>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_2<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)4>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_2<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_2<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_2<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_2<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_2<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>, (bool)1>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_2<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_3<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)4>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_3<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)4>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_3<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)3>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_3<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)4>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_3<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_3<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_3<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_3<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_3<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>, (bool)1>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_3<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)128, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)4>, (bool)0>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)4>, (bool)0>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)256, (int)64, (int)32, (int)4, (int)1, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)3>, (bool)0>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Ampere_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Ampere, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)4>, (bool)0>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)128, (int)64, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>, (bool)0>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)128, (int)32, (int)4, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>, (bool)0>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)256, (int)32, (int)32, (int)4, (int)1, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>, (bool)0>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)128, (int)32, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>, (bool)0>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)32, (int)64, (int)2, (int)1, (int)2, (int)1>, (bool)1>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>, (bool)0>(T1::Params)
void xmma_cudnn::implicit_gemm::strided_dgrad_indexed::kernel<xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Kernel_traits<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_a<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>>, xmma_cudnn::implicit_gemm::strided_dgrad_indexed::Gmem_tile_epilogue<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, xmma_cudnn::Fragment_c<xmma_cudnn::Volta_hmma_fp32_traits, xmma_cudnn::Cta_tile<xmma_cudnn::Volta<(int)0>, (int)64, (int)64, (int)64, (int)2, (int)2, (int)1, (int)1>, (bool)0>>, xmma_cudnn::implicit_gemm::Input_related<(int)0, (int)0, (int)0, (bool)0>, (int)1>, (bool)0>(T1::Params)
volta_fp16_s884cudnn_fp16_128x128_ldg8_dgrad_Dgrad1x1_interior_nchw_nt_v1
volta_fp16_s884cudnn_fp16_128x128_ldg8_dgrad_f2f_exp_large_nhwc2nchw_tt_v1
volta_fp16_s884cudnn_fp16_128x128_ldg8_dgrad_f2f_exp_medium_nhwc2nchw_tt_v1
volta_fp16_s884cudnn_fp16_128x128_ldg8_dgrad_f2f_exp_small_nhwc2nchw_tt_v1
volta_fp16_s884cudnn_fp16_128x128_ldg8_splitK_dgrad_f2f_exp_small_nhwc_tt_v1
volta_fp16_s884cudnn_fp16_256x128_ldg8_dgrad_Dgrad1x1_interior_nchw_nt_v1
volta_fp16_s884cudnn_fp16_256x128_ldg8_dgrad_f2f_exp_medium_nhwc2nchw_tt_v1
volta_fp16_s884cudnn_fp16_256x128_ldg8_dgrad_f2f_exp_small_nhwc2nchw_tt_v1
volta_fp16_s884cudnn_fp16_256x64_ldg8_dgrad_f2f_exp_small_nhwc2nchw_tt_v1
volta_fp16_s884gemm_fp16_128x128_ldg8_f2f_nn
volta_fp16_s884gemm_fp16_128x128_ldg8_f2f_nt
volta_fp16_s884gemm_fp16_128x128_ldg8_f2f_stages_32x1_nt
volta_fp16_s884gemm_fp16_128x128_ldg8_f2f_stages_32x1_tn
volta_fp16_s884gemm_fp16_128x128_ldg8_f2f_tn
volta_fp16_s884gemm_fp16_128x256_ldg8_f2f_nt
volta_fp16_s884gemm_fp16_128x64_ldg8_f2f_nn
volta_fp16_s884gemm_fp16_128x64_ldg8_f2f_tn
volta_fp16_s884gemm_fp16_256x128_ldg8_f2f_nn
volta_fp16_s884gemm_fp16_256x128_ldg8_f2f_nt
volta_fp16_s884gemm_fp16_256x128_ldg8_f2f_tn
volta_fp16_s884gemm_fp16_256x64_ldg8_f2f_nn
volta_fp16_s884gemm_fp16_256x64_ldg8_f2f_nt
volta_fp16_s884gemm_fp16_512x64_ldg8_f2f_nt
volta_fp16_s884gemm_fp16_64x128_ldg8_f2f_nn
volta_fp16_s884gemm_fp16_64x128_ldg8_f2f_nt
volta_fp16_s884gemm_fp16_64x128_ldg8_f2f_tn
volta_fp16_s884gemm_fp16_64x64_ldg8_f2f_nn
volta_fp16_s884gemm_fp16_64x64_ldg8_f2f_nt
volta_fp16_s884gemm_fp16_64x64_ldg8_f2f_tn
volta_fp16_scudnn_fp16_128x128_stridedB_small_nn_v1
volta_fp16_sgemm_fp16_128x128_nt
volta_fp16_sgemm_fp16_128x32_nn
volta_fp16_sgemm_fp16_128x32_nt
volta_fp16_sgemm_fp16_128x32_tn
volta_fp16_sgemm_fp16_128x64_nn
volta_fp16_sgemm_fp16_128x64_nt
volta_fp16_sgemm_fp16_128x64_tn
volta_fp16_sgemm_fp16_128x64_tt
volta_fp16_sgemm_fp16_32x128_nn
volta_fp16_sgemm_fp16_32x128_nt
volta_fp16_sgemm_fp16_32x128_tn
volta_fp16_sgemm_fp16_32x32_sliced1x4_nt
volta_fp16_sgemm_fp16_64x32_sliced1x4_nt
volta_gcgemm_32x32_nt
volta_gcgemm_64x32_nt
volta_s884cudnn_fp16_128x128_ldg8_wgrad_exp_interior_nhwc_nt_v1
volta_s884cudnn_fp16_128x128_ldg8_wgrad_idx_exp_interior_nhwc_nt_v1
volta_s884cudnn_fp16_256x128_ldg8_wgrad_idx_exp_interior_nhwc_nt_v1
volta_s884cudnn_fp16_64x64_sliced1x4_ldg8_wgrad_idx_exp_interior_nhwc_nt
volta_s884gemm_fp16_64x64_ldg8_nn
volta_s884gemm_fp16_64x64_ldg8_tn
